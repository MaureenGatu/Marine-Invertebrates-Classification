{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Marine Invertebrates Classification Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LtIaqJiQL0SF",
        "fxjMDaP8ldtU",
        "ZymkFN1zrNeX",
        "FfmF_BrOC-jL",
        "K_6bNgQCsirK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaureenGatu/Marine-Invertebrates-Classification/blob/main/Marine_Invertebrates_Classification_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wde-_TbrsEbM"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_tX2t1ntfGz"
      },
      "source": [
        "%%capture\n",
        "!pip install split-folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfifFO_tsB9u",
        "outputId": "2964e168-0116-4ce6-cf3e-67668fd42a85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc2Io2qcSI-G"
      },
      "source": [
        "#loading libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#keras libraries\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#libraries for loading images\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "import cv2\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-lp9-2Qb__E"
      },
      "source": [
        "#split our training data into train, test and validation set.\n",
        "train_dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Augmented/'\n",
        "# split_dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/'\n",
        "\n",
        "# #Split the data\n",
        "# import splitfolders\n",
        "\n",
        "# #split the data with a 70/20/10 ratio\n",
        "# splitfolders.ratio(train_dir,output = split_dir, seed = 100, ratio = (0.7,0.3), group_prefix = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-y3ZKdzNQI5"
      },
      "source": [
        "# 'Actiniaria', 'Actinoptilum_molle', 'Actinoscyphia_plebeia', 'Actinostola_capensis', 'Aequorea_spp',\n",
        "# 'Africolaria_rutila',\n",
        "#  'Alcyonacea', 'Amalda_bullioides', 'Anthoptilum_grandiflorum', 'Aphelodoris_sp_', \n",
        "# 'Aphrodita_alta', 'Aristeus_varidens',\n",
        "#['Ascidiacea', 'Astropecten_irregularis_pontoporeus', 'Athleta_abyssicola', 'Athleta_lutosa', 'Bolocera_kerguelensis', \n",
        "#'Brissopsis_lyrifera_capensis', 'Bryozoa', 'Cavernularia_spp', 'Cephalodiscus_gilchristi',]\n",
        "# ['Ceramaster_patagonicus_euryplax', 'Charonia_lampas', 'Cheilostomatida', 'Cheiraster_hirsutus', 'Chondraster_elattosis', 'Chrysaora_fulgida', 'Chrysaora_spp', 'Comanthus_wahlbergii', 'Comitas_saldanhae', 'Comitas_stolida', \n",
        "#'Cosmasterias_felipes', 'Crossaster_penicillatus', 'Cypraeovula_iutsui', 'Diplopteraster_multipes', \n",
        "#'Dipsacaster_sladeni_capensis']\n",
        "# ['Echinus_gilchristi', 'Eleutherobia_variable', 'Euspira_napus', 'Exodromidia_spinosa',\n",
        "# 'Exodromidia_spinosissima', 'Flabellum_(Ulocyathus)_messum', 'Funchalia_woodwardi', 7\n",
        "#'Fusinus_africanae', 'Fusinus_hayesi', 'Fusitriton_magellanicus', 'Fusivoluta_pyrrhostoma', \n",
        "#'Glyphocrangon_spp', 'Goneplax_clevai', 'Granulifusus_rubrolineatus', 'Gynandrocarpa_placenta', \n",
        "#'Halcurias_capensis', 'Haliporoides_triarthrus', 'Hemiocnus_insolens',\n",
        "# 'Henricia_abyssalis', 'Hermit_crab', 'Hippasteria_phrygiana', 'Holothuroidea', 'Hydrozoa_spp']#, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNePzEnhG6eb"
      },
      "source": [
        "fish = ['Rochinia_hertwigi', 'Salpa_spp_', 'Scaphander_punctostriatus', 'Scleractinia', 'Sclerasterias_spp', 'Seafan', 'Solenocera_africana',  'Stereomastis_sculpta', 'Stylasteridae', 'Suberites_dandelenae', 'Sympagurus_dimorphus', 'Terebratulina_sp_', 'Turritella_declivis', 'Vitjazmaia_latidactyla']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwgCrDrtUfcT",
        "outputId": "d84dd9cb-ffac-448b-b45a-56f45fad8749"
      },
      "source": [
        "ls1 = ['Pasiphaea_sp._1' ,'Pecten_sulcicostatus','Philinopsis_capensis', 'Polyechinus_agulhensis', 'Pseudarchaster_tessellatus',\n",
        "       'Pseudostichopus_langeae', 'Pterygosquilla_capensis', 'Rossella_antarctica']\n",
        "\n",
        "ls2 = ['Pycnogonid_spp_', 'Spatangus_capensis', 'Synallactes_viridilimus', 'Toraster_tuberculatus','Triviella_spp_']\n",
        "#, 'Turritella_declivis\n",
        "print(len(ls1))\n",
        "print(len(ls2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5dbUvqASNiZ",
        "outputId": "d8271578-ea8b-4a8c-a90e-a43503244572"
      },
      "source": [
        "check = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Augmented'\n",
        "check_dir= pathlib.Path(check)\n",
        "image_counts = len(list(check_dir.glob('Pycnogonid_spp_/*')))\n",
        "print('Prawns' , image_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prawns 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG_O91_hGtnp"
      },
      "source": [
        "# Organize data into train, valid, test dirs  3076\n",
        "import shutil\n",
        "import random\n",
        "#diseases = ['Blight', 'Common_rust', 'Gray Leaf Spot', 'SMUT500', 'healthy', 'maizestreak_aug']\n",
        "os.chdir('/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Augmented/')\n",
        "# if os.path.isdir('train/0/') is False: \n",
        "#     os.mkdir('train')\n",
        "#     os.mkdir('valid')\n",
        "#     os.mkdir('test')\n",
        "\n",
        "for i in ls2:\n",
        "    shutil.move(f'{i}', 'train')\n",
        "    os.mkdir(f'valid/{i}')\n",
        "    os.mkdir(f'test/{i}')\n",
        "\n",
        "    valid_samples = random.sample(os.listdir(f'train/{i}'), 60)\n",
        "    for j in valid_samples:\n",
        "        shutil.move(f'train/{i}/{j}', f'valid/{i}')\n",
        "\n",
        "    test_samples = random.sample(os.listdir(f'train/{i}'), 30)\n",
        "    for k in test_samples:\n",
        "        shutil.move(f'train/{i}/{k}', f'test/{i}')\n",
        "os.chdir('../..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXE8jQFhPw7J"
      },
      "source": [
        "# Get more information about the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cJTlaOCP1JC",
        "outputId": "1c79d0c9-f929-4117-9006-3bbf5d4b9cdf"
      },
      "source": [
        "import os\n",
        "dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Augmented/valid'\n",
        "lista = os.listdir(dir) # dir is your directory path\n",
        "number_files = len(lista)\n",
        "print(number_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tTkRIESwouE"
      },
      "source": [
        "dirb = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Marine Invertebrates Data/test_small/test_small'\n",
        "listb = os.listdir(dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnddPhvWwk5c",
        "outputId": "ac84d4e1-36fb-45a3-f6ae-c3f7d507ad20"
      },
      "source": [
        "listb == lista"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3j4eUBdvCtX",
        "outputId": "d950f32b-90ba-4437-a031-03a1cb26ddf6"
      },
      "source": [
        "kdir= pathlib.Path(dir)\n",
        "image_counts = len(list(kdir.glob('Triviella_spp_/*')))\n",
        "print('Validation data',image_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwfH5bK1RxMj"
      },
      "source": [
        "#path to get document from drive.\n",
        "p = Path(\"/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Marine Invertebrates Data/train_small/train_small\") \n",
        "#Getting the directory to the train,validation and test data\n",
        "train_path2 = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Split data/train'\n",
        "valid_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/val'\n",
        "test_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/test'\n",
        "# downloading our dataset\n",
        "\n",
        "data_dir= pathlib.Path(train_path2)\n",
        "data_dir2 = pathlib.Path(valid_path)\n",
        "data_dir3 = pathlib.Path(test_path)\n",
        "\n",
        "# The total number of images present\n",
        "image_counts = len(list(data_dir.glob('*/*')))\n",
        "print('Training data',image_counts)\n",
        "\n",
        "image_counts = len(list(data_dir2.glob('*/*')))\n",
        "print('Validation data',image_counts)\n",
        "\n",
        "image_counts = len(list(data_dir3.glob('*/*')))\n",
        "print('Test data',image_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gflc8HyeU6S-",
        "outputId": "16a37ae9-a300-4b05-9084-6ae4af097635"
      },
      "source": [
        "# The total number of validation images present\n",
        "#data_dir= pathlib.Path(valid_path)\n",
        "\n",
        "# image_counts = len(list(data_dir.glob('Actinoscyphia_plebeia/*')))\n",
        "# print('Actinaria' , image_counts)\n",
        "# image_counts = len(list(data_dir.glob('Bryozoa/*')))\n",
        "# print('Bryozoa' , image_counts)\n",
        "# image_counts = len(list(data_dir.glob('Isididae/*')))\n",
        "# print('Isididae' , image_counts)\n",
        "# image_counts = len(list(data_dir.glob('Isopods/*')))\n",
        "# print('Isopods' , image_counts)\n",
        "# image_counts = len(list(data_dir.glob('Philine_aperta/*')))\n",
        "# print('Philine_aperta' , image_counts)\n",
        "image_counts = len(list(data_dir.glob('Triviella_spp_/*')))\n",
        "print('Prawns' , image_counts)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prawns 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0J_FqcjrdiK"
      },
      "source": [
        "##  Data Augmentation (Mark's Code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFm11bMqHWS",
        "outputId": "4cc039bf-b520-4424-c631-d37e7a79743c"
      },
      "source": [
        "#Repeat for all classes(ps. look for a for loop to do this next time)\n",
        "\n",
        "import albumentations as A\n",
        "#from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "#from bbaug import policies\n",
        "import cv2\n",
        "import os\n",
        "#from pascal_voc_writer import Writer\n",
        "from xml.dom import minidom\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "imagespath = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Split data/train/Toraster_tuberculatus/'\n",
        "writepath = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Augmented/Toraster_tuberculatus/'\n",
        "random.seed(7)\n",
        "def readImage(filename):\n",
        "    # OpenCV uses BGR channels\n",
        "    img = cv2.imread(imagespath+filename)\n",
        "    return img\n",
        "def start():\n",
        "    count = 3000\n",
        "    for filename in sorted(os.listdir(imagespath)):\n",
        "        if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\"):\n",
        "            title, ext = os.path.splitext(os.path.basename(filename))\n",
        "            image = readImage(filename)\n",
        "            for i in tqdm(range(0, 18)):\n",
        "                img = copy.deepcopy(image)\n",
        "                transform = getTransform(i)\n",
        "                try:\n",
        "                    transformed = transform(image=img, )\n",
        "                    transformed_image = transformed['image']\n",
        "                    name = title+str(count)+'.jpg'\n",
        "                    cv2.imwrite(writepath+name, transformed_image)\n",
        "                    count = count+1\n",
        "                except:\n",
        "                    print(\"no image has been augmented\")\n",
        "                    pass\n",
        "def getTransform(loop):\n",
        "    # Compose a complex augmentation pipeline\n",
        "    augmentation_pipeline = A.Compose(\n",
        "        [\n",
        "            A.Resize(224,224, p=1), # Resize all images to 224\n",
        "            A.HorizontalFlip(p=0.5),  # apply horizontal flip to 50% of images\n",
        "            A.VerticalFlip(p=.5), # Vertical flip 50% of the images\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    # apply one of transforms to 50% of images\n",
        "                    A.RandomContrast(),  # apply random contrast\n",
        "                    A.RandomGamma(),  # apply random gamma\n",
        "                    A.RandomBrightness(),  # apply random brightness\n",
        "                ],\n",
        "                p=1.0\n",
        "            ),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    # apply one of transforms to 50% images\n",
        "                    A.ElasticTransform(\n",
        "                        alpha=120,\n",
        "                        sigma=120 * 0.05,\n",
        "                        alpha_affine=120 * 0.03\n",
        "                    ),\n",
        "                    A.GridDistortion(),\n",
        "                    A.OpticalDistortion(\n",
        "                        distort_limit=2,\n",
        "                        shift_limit=0.5\n",
        "                    ),\n",
        "                ],\n",
        "                p=.5\n",
        "            )\n",
        "        ],\n",
        "        p=1\n",
        "    )\n",
        "    return augmentation_pipeline\n",
        "start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:00<00:00, 89.84it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 49.92it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 47.92it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 56.56it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 49.05it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 57.36it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 60.90it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 53.29it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 51.86it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 54.95it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 52.46it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 50.28it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 50.17it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 50.56it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 52.78it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 72.03it/s] \n",
            "100%|██████████| 18/18 [00:00<00:00, 52.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ywg3lFDN_2"
      },
      "source": [
        "# Training with the augmented data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1obilGMDfzq"
      },
      "source": [
        "## Basic CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dzv9cqJDSHg"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohTf3AsyalWE"
      },
      "source": [
        "###Data Preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaPZDhbmaqxp"
      },
      "source": [
        "#creating data and resizing our images\n",
        "batch_size = 50\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GAa45INfTxd"
      },
      "source": [
        "#augment our image to largen the number of the dataset.\n",
        "\n",
        "train_path2 = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Split data/train'\n",
        "valid_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Split data/val'\n",
        "test_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/Split data/test'\n",
        "\n",
        "data_dir= pathlib.Path(train_path2)\n",
        "data_dir2 = pathlib.Path(valid_path)\n",
        "data_dir3 = pathlib.Path(test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9WmFAwCm_pK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYbeRLLshDjQ",
        "outputId": "a9b9dbb0-7b23-4d31-9ab6-f95080363c4a"
      },
      "source": [
        "#validation set\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir2,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "#Traing data\n",
        "#training set train_ds val_ds\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "#Test Data\n",
        "#training set train_ds val_ds\n",
        "test_path = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir3,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 574 files belonging to 137 classes.\n",
            "Found 2120 files belonging to 137 classes.\n",
            "Found 417 files belonging to 137 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13kI8RzaLTGP",
        "outputId": "af9c9a82-97cd-4be0-c153-bc22274267f4"
      },
      "source": [
        "# Class name attributes on our dataset\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Actiniaria', 'Actinoptilum_molle', 'Actinoscyphia_plebeia', 'Actinostola_capensis', 'Aequorea_spp', 'Africolaria_rutila', 'Alcyonacea', 'Amalda_bullioides', 'Anthoptilum_grandiflorum', 'Aphelodoris_sp_', 'Aphrodita_alta', 'Aristeus_varidens', 'Armina_sp_', 'Ascidiacea', 'Astropecten_irregularis_pontoporeus', 'Athleta_abyssicola', 'Athleta_lutosa', 'Bolocera_kerguelensis', 'Brissopsis_lyrifera_capensis', 'Bryozoa', 'Cavernularia_spp', 'Cephalodiscus_gilchristi', 'Ceramaster_patagonicus_euryplax', 'Charonia_lampas', 'Cheilostomatida', 'Cheiraster_hirsutus', 'Chondraster_elattosis', 'Chrysaora_fulgida', 'Chrysaora_spp', 'Comanthus_wahlbergii', 'Comitas_saldanhae', 'Comitas_stolida', 'Cosmasterias_felipes', 'Crossaster_penicillatus', 'Cypraeovula_iutsui', 'Diplopteraster_multipes', 'Dipsacaster_sladeni_capensis', 'Echinus_gilchristi', 'Eleutherobia_variable', 'Euspira_napus', 'Exodromidia_spinosa', 'Exodromidia_spinosissima', 'Flabellum_(Ulocyathus)_messum', 'Funchalia_woodwardi', 'Fusinus_africanae', 'Fusinus_hayesi', 'Fusitriton_magellanicus', 'Fusivoluta_pyrrhostoma', 'Glyphocrangon_spp', 'Goneplax_clevai', 'Granulifusus_rubrolineatus', 'Gynandrocarpa_placenta', 'Halcurias_capensis', 'Haliporoides_triarthrus', 'Hemiocnus_insolens', 'Henricia_abyssalis', 'Hermit_crab', 'Hippasteria_phrygiana', 'Holothuroidea', 'Hydrozoa_spp', 'Inachidae', 'Isididae', 'Isopods', 'Kaloplocamus_ramosus', 'Lamellaria_Coriocella_spp', 'Limopsis_chuni', 'Lithodes_ferox', 'Lophaster_quadrispinus', 'Luidia_sarsii_africana', 'Marginella_musica', 'Marthasterias_africana', 'Mediaster_bairdi_capensis', 'Merhippolyte_agulhasensis', 'Miersiograpsus_kingsleyi', 'Munida_benguela', 'Mursia_cristiata', 'Mycale_anisochela', 'Nassarius speciosus', 'Nassarius_vinctus', 'Neolithodes_asperrimus', 'Neopilumnoplax_heterochir', 'Neptuneopsis_gilchristi', 'Nudibranchia', 'Ophiomyxa_vivipara_capensis', 'Ophiothrix_aristulata', 'Ophiothrix_fragilis', 'Ophiothrix_spp', 'Ophiura_costata_costata', 'Ophiura_trimeni', 'Pagurus_cuanensis', 'Parapagurus_bouvieri', 'Pasiphaea_sp._1', 'Pasiphaea_sp._2', 'Pecten_sulcicostatus', 'Pelagia_noctiluca', 'Perissasterias_polyacantha', 'Philine_aperta', 'Philinopsis_capensis', 'Phormosoma_placenta_africana', 'Plesionika_martia', 'Pleurobranchaea_bubala', 'Polychaete_tubes_(only)', 'Polychaete_worms', 'Polyechinus_agulhensis', 'Poraniopsis_echinaster', 'Porifera', 'Prawns', 'Projasus_parkeri', 'Pseudarchaster_brachyactis', 'Pseudarchaster_tessellatus', 'Pseudodromia_rotunda', 'Pseudodromia_spp_', 'Pseudostichopus_langeae', 'Psilaster_acuminatus', 'Pteraster_capensis', 'Pterygosquilla_capensis', 'Pycnogonid_spp_', 'Pyromaia_tuberculata', 'Rochinia_hertwigi', 'Rossella_antarctica', 'Salpa_spp_', 'Scaphander_punctostriatus', 'Scleractinia', 'Sclerasterias_spp', 'Seafan', 'Solenocera_africana', 'Spatangus_capensis', 'Stereomastis_sculpta', 'Stylasteridae', 'Suberites_dandelenae', 'Sympagurus_dimorphus', 'Synallactes_viridilimus', 'Terebratulina_sp_', 'Toraster_tuberculatus', 'Triviella_spp_', 'Turritella_declivis', 'Vitjazmaia_latidactyla']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISFXjbU2LuKj",
        "outputId": "355092a2-8e3e-4667-d73a-5d5ace819c12"
      },
      "source": [
        "# Iterating over our dataset to retrieve images\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50, 224, 224, 3)\n",
            "(50,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7FcCagMATm7"
      },
      "source": [
        "# import mxnet as mx \n",
        "# aug = mx.image.HorizontalFlipAug(p=1)\n",
        "# aug_image = aug(Common_rust)\n",
        "# plot_mx_array(aug_image)\n",
        "\n",
        "# assert example_image.shape == (427, 640, 3)\n",
        "# assert aug_image.shape == (427, 640, 3)\n",
        "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "#   train_path,\n",
        "#   image_size=(224,224),\n",
        "#   batch_size=10)\n",
        "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "#   valid_path,\n",
        "#   image_size=(224,224),\n",
        "#   batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP_eI40P2_UP"
      },
      "source": [
        "# #augmentation.\n",
        "# data_augmentation = keras.Sequential(\n",
        "#   [\n",
        "#     layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "#                                                  input_shape=(img_height, \n",
        "#                                                               img_width,\n",
        "#                                                               3)),\n",
        "#     layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "#     layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "#   ]\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFEhtFKP6keC"
      },
      "source": [
        "# #visualize image that has been augumented\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, _ in train_ds.take(2):\n",
        "#   for i in range(9):\n",
        "#     augmented_images = data_augmentation(images)\n",
        "#     ax = plt.subplot(3, 3, i + 1)\n",
        "#     plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "#     plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SIa-_xm-Q69"
      },
      "source": [
        "# Cache the data so that it is saved in the first epoch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIaqJiQL0SF"
      },
      "source": [
        "### Intergrate wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KThm6jF7Lz-d"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sXDdFDziM4fT",
        "outputId": "2f19ac38-cd44-4222-8bd1-b5f6c2215a92"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaureengatu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxjMDaP8ldtU"
      },
      "source": [
        "## **Initial Baseline CNN Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5EUXDmqlrSF"
      },
      "source": [
        "Our number one concern should be **overfitting**. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VptEgIdGl7rN"
      },
      "source": [
        "* The main focus for fighting overfitting should be the **entropic capacity** of our model --how much information the model is allowed to store.\n",
        "    * A model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.\n",
        "* One ways to modulate entropic capacity is the choice of the **number of parameters in your model**, i.e. number and size of each layer. \n",
        "* Another way is the **use of weight regularization**, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
        "* **Dropout** also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PqP6ZyliS-"
      },
      "source": [
        "# Libraries for the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0nslZiMnr-F"
      },
      "source": [
        "#Building our baseline model\n",
        "#Compile the the model\n",
        "num_classes = 137\n",
        "\n",
        "simple_model = Sequential([\n",
        "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(1024, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(2048, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(4096, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  # layers.Conv2D(2048, 3, padding='same', activation='relu'),\n",
        "  # layers.MaxPooling2D(),\n",
        "  # layers.Conv2D(1024, 3, padding='same', activation='relu'),\n",
        "  # layers.MaxPooling2D(),\n",
        "  # layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "  # layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(137, activation='softmax'),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Dense(num_classes)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgSqd7pvPeUI"
      },
      "source": [
        "### Initialize your wandb run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "MruKzWu-Pkgy",
        "outputId": "15bcd37f-a443-4a5d-9288-7c41c6c101b6"
      },
      "source": [
        "# Initialize wandb with your project name\n",
        "run = wandb.init(project='Marine Invertebrates Classification Challenge.',save_code= True, name='simple_cnn_originaldata',\n",
        "                 config={  # and include hyperparameters and metadata\n",
        "                     \"learning_rate\": 0.005,\n",
        "                     \"epochs\": 6,\n",
        "                     \"batch_size\": 50,\n",
        "                     \"layers\": 4,\n",
        "                     \"optimizer\": 'Adam',\n",
        "                     \"loss_function\": \"sparse_categorical_crossentropy\", #CategoricalCrossentropy\n",
        "                     \"architecture\": \"CNN\",\n",
        "                     \"dataset\": \"Maize Crop images\"\n",
        "                 })\n",
        "config = wandb.config  # We'll use this to configure our experiment\n",
        "\n",
        "# Initialize model like you usually do.\n",
        "tf.keras.backend.clear_session()\n",
        "#model = Model()\n",
        "simple_model.summary()\n",
        "\n",
        "# Compile model like you usually do.\n",
        "# Notice that we use config, so our metadata matches what gets executed\n",
        "optimizer = tf.keras.optimizers.Adam(config.learning_rate) \n",
        "simple_model.compile(optimizer, config.loss_function, metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaureengatu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/maureengatu/Marine%20Invertebrates%20Classification%20Challenge./runs/2qllfhq4\" target=\"_blank\">simple_cnn_originaldata</a></strong> to <a href=\"https://wandb.ai/maureengatu/Marine%20Invertebrates%20Classification%20Challenge.\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling (Rescaling)        (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 224, 224, 128)     3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 512)     590336    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 56, 1024)      4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 1024)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 2048)      18876416  \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 2048)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 4096)      75501568  \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 4096)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 200704)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 137)               27496585  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 137)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 137)               18906     \n",
            "=================================================================\n",
            "Total params: 127,207,011\n",
            "Trainable params: 127,207,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlk_R4RPqPl3",
        "outputId": "3f832801-d1d6-4be3-dc04-55a59fb8f278"
      },
      "source": [
        "# #fit the model\n",
        "history = simple_model.fit(train_ds,\n",
        "          epochs=config.epochs, \n",
        "          batch_size=config.batch_size,\n",
        "          validation_data=val_ds,\n",
        "          callbacks=[WandbCallback()])\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            " 2/43 [>.............................] - ETA: 3:39:39 - loss: 10.7290 - acc: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-29:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 215, in check_status\n",
            "    status_response = self._interface.communicate_stop_status()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 737, in communicate_stop_status\n",
            "    resp = self._communicate(req, timeout=timeout, local=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 539, in _communicate\n",
            "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 544, in _communicate_async\n",
            "    raise Exception(\"The wandb backend process has shutdown\")\n",
            "Exception: The wandb backend process has shutdown\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2HErvyBFUlN"
      },
      "source": [
        "# #Create plots of loss and accuracy on the training and validation sets\n",
        "\n",
        "# acc = history.history['acc']\n",
        "# val_acc = history.history['val_acc']\n",
        "\n",
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "\n",
        "# epochs_range = range(epochs)\n",
        "\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "# plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "# plt.legend(loc='lower right')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(epochs_range, loss, label='Training Loss')\n",
        "# plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.title('Training and Validation Loss')\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZymkFN1zrNeX"
      },
      "source": [
        "## Mobilenet \n",
        "    (Train/Validation/Test Split)\n",
        "  \n",
        "  [encoding](https://www.damienpontifex.com/posts/images-with-directories-as-labels-for-tensorflow-data/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X57bG4o74rAU"
      },
      "source": [
        "# # Organize data into train, valid, test dirs  3076\n",
        "\n",
        "# diseases = ['Blight', 'Common_rust', 'Gray Leaf Spot', 'SMUT500', 'healthy', 'maizestreak_aug']\n",
        "# os.chdir('/content/drive/MyDrive/MOG/DATA/Datasets/Augment Images')\n",
        "# if os.path.isdir('train/0/') is False: \n",
        "#     os.mkdir('train')\n",
        "#     os.mkdir('valid')\n",
        "#     os.mkdir('test')\n",
        "\n",
        "#     for i in diseases:\n",
        "#         shutil.move(f'{i}', 'train')\n",
        "#         os.mkdir(f'valid/{i}')\n",
        "#         os.mkdir(f'test/{i}')\n",
        "\n",
        "#         valid_samples = random.sample(os.listdir(f'train/{i}'), 100)\n",
        "#         for j in valid_samples:\n",
        "#             shutil.move(f'train/{i}/{j}', f'valid/{i}')\n",
        "\n",
        "#         test_samples = random.sample(os.listdir(f'train/{i}'), 20)\n",
        "#         for k in test_samples:\n",
        "#             shutil.move(f'train/{i}/{k}', f'test/{i}')\n",
        "# os.chdir('../..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfmF_BrOC-jL"
      },
      "source": [
        "### MobileNet Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KwYj3KDPzy"
      },
      "source": [
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Activation,GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "tf.random.set_seed(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOfzc_WUsdFK",
        "outputId": "eb093516-5c2d-43b8-fb36-7db6f88245a5"
      },
      "source": [
        "#Getting the directory to the train,validation and test data\n",
        "train_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/train'\n",
        "valid_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/val'\n",
        "test_path = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/test'\n",
        "\n",
        "\n",
        "#Preprocessing the data for MobileNet architecture\n",
        "\n",
        "#Train data\n",
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=train_path, target_size=(224,224), batch_size=10)\n",
        "\n",
        "\n",
        "#Validation data\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=valid_path, target_size=(224,224), batch_size=10)\n",
        "\n",
        "#Test data\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
        "    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 21935 images belonging to 137 classes.\n",
            "Found 6244 images belonging to 137 classes.\n",
            "Found 3227 images belonging to 137 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dFi7_cVD_2C"
      },
      "source": [
        "### Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU7iO-WrD_2E"
      },
      "source": [
        "#download the MobileNet model\n",
        "mobile = tf.keras.applications.mobilenet.MobileNet()\n",
        "\n",
        "#summary of the model\n",
        "mobile.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9b6IxmD_2F"
      },
      "source": [
        "* We'll be using this to build a new model. This new model will consist of the original MobileNet up to the sixth to last layer. We're not including the last five layers of the original MobileNet.\n",
        "\n",
        "* By looking at the summary of the original model, we can see that by not including the last five layers, we'll be including everything up to and including the last global_average_pooling layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXjzYWEDD_2G"
      },
      "source": [
        "#Next, we're going to grab the output from the sixth to last layer of the model and store it in this variable x.\n",
        "#his setup, we'll be keeping the vast majority of the original MobileNet architecutre, which has 88 layers total.\n",
        "\n",
        "x = mobile.layers[-6].output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KxNYeJGD_2H"
      },
      "source": [
        "#### **create an output layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-uO8N0D_2H"
      },
      "source": [
        "#create an output layer that we're calling output, \n",
        "#which will just be a Dense layer with 6 output nodes for the ten corresponding classes,\n",
        "# we'll use the softmax activation function.\n",
        "\n",
        "output = Dense(units=137, activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNRtPluAD_2I"
      },
      "source": [
        "#construct the new fine-tuned model\n",
        "model = keras.Model(inputs=mobile.input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzx_pdIgD_2J"
      },
      "source": [
        "* To build the new model, we create an instance of the Model class and specify the inputs to the model to be equal to the input of the original MobileNet\n",
        "* Then we define the outputs of the model to be equal to the output variable we created directly above.\n",
        "* This creates a new model, which is identical to the original MobileNet up to the original model's sixth to last layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awEESG4OD_2J"
      },
      "source": [
        "#choose how many layers we actually want to be trained when we train on our new data set.\n",
        "\n",
        "#we train the last 10 layers\n",
        "for layer in model.layers[:-10]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fCZmKseD_2K"
      },
      "source": [
        "* So the twenty-third-to-last layer and all layers after it will be trained when we fit the model on the new data set. All layers above will not be trained, so their original ImageNet weights will stay in place.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZzwfz91D_2L"
      },
      "source": [
        "#Looking at the model summary to see the new model architecture, along with how the number of trainable parameters.\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFBe8lThD_2L",
        "outputId": "75fc7650-880b-4788-9a15-bdaca85d2054"
      },
      "source": [
        "#compile the model\n",
        "#wandb.init(project='CNN Classification Model',save_code= True, name='MobileNet_basemodel') \n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer, loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "T7SJuZuvD_2M",
        "outputId": "a49fc72d-c32d-4333-9ef1-0b30c1bbd20c"
      },
      "source": [
        "# fit() to train the model '.',save_code= True, name='simple_cnn_originaldata',\n",
        "run = wandb.init(project='Marine Invertebrates Classification Challenge',save_code= True, name='MobileNet_Baldata_model',\n",
        "            config={  # and include hyperparameters and metadata\n",
        "                     \"learning_rate\": 0.0001,\n",
        "                     \"epochs\": 10,\n",
        "                    \"optimizer\":\"Adam\",\n",
        "                    \"metric\": 'Accuracy',\n",
        "                     \"loss_function\": \"categorical_crossentropy\", #CategoricalCrossentropy\n",
        "                     \"architecture\": \"ModelNet Model\",\n",
        "                     \"dataset\": \"Maize Crop images\"\n",
        "                 })\n",
        "config = wandb.config \n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=config.learning_rate)\n",
        "model.compile(optimizer, loss=config.loss_function, metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(x=train_batches,\n",
        "            steps_per_epoch=len(train_batches),\n",
        "            validation_data=valid_batches,\n",
        "            validation_steps=len(valid_batches),\n",
        "            epochs=config.epochs, callbacks=[WandbCallback()],\n",
        "            verbose=2\n",
        ")\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/maureengatu/Marine%20Invertebrates%20Classification%20Challenge/runs/3d3zeg0s\" target=\"_blank\">MobileNet_Baldata_model</a></strong> to <a href=\"https://wandb.ai/maureengatu/Marine%20Invertebrates%20Classification%20Challenge\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2194/2194 - 2552s - loss: 1.1946 - acc: 0.7618 - val_loss: 0.2761 - val_acc: 0.9488\n",
            "Epoch 2/10\n",
            "2194/2194 - 1229s - loss: 0.2488 - acc: 0.9571 - val_loss: 0.1313 - val_acc: 0.9734\n",
            "Epoch 3/10\n",
            "2194/2194 - 1245s - loss: 0.1131 - acc: 0.9847 - val_loss: 0.0875 - val_acc: 0.9797\n",
            "Epoch 4/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWJAv5W6Pze1"
      },
      "source": [
        "### Model confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-nrBSGsOvL5"
      },
      "source": [
        "test_labels = test_batches.classes\n",
        "predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "4f3963686aca497c99d5521a0b4fabf6"
          ]
        },
        "id": "511Pa3diO3PU",
        "outputId": "5a746f75-839c-4fd8-cc44-5f1b26774614"
      },
      "source": [
        "run = wandb.init(project='CNN Classification Model',save_code= True, name='MobileNet_Bal_model')\n",
        "\n",
        "cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))\n",
        "wandb.log({'confusion_matrix':cm})\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">MobileNet_Bal_model</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2kn92u8l\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2kn92u8l</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210926_212527-2kn92u8l</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1434<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f3963686aca497c99d5521a0b4fabf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.29MB of 0.29MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210926_212527-2kn92u8l/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210926_212527-2kn92u8l/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">MobileNet_Bal_model</strong>: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2kn92u8l\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2kn92u8l</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ed6OcQqPKwa"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "test_batches.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiBbTsFqPXBK"
      },
      "source": [
        "cm_plot_labels = ['0','1','2','3','4','5']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5OBXGmo4bGn"
      },
      "source": [
        "#Classification report\n",
        "print(classification_report(y_true=test_labels, y_pred=predictions.argmax(axis=1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeJuYGPwAhNb"
      },
      "source": [
        "# make a prediction with the test dataset\n",
        "for images_batch, labels_batch in test_path.take(1):\n",
        "\n",
        "  plt.imshow(images_batch[0].numpy().astype('uint8')) # print(images_batch[0]) returns tensors. Use .numpy() to change to numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Zg3Ax_CJtG"
      },
      "source": [
        "# Import numpy\n",
        "import numpy as np\n",
        "\n",
        "# make a prediction with the test dataset\n",
        "for images_batch, labels_batch in test_path.take(1):\n",
        "\n",
        "  first_image = images_batch[0].numpy().astype('uint8')\n",
        "  first_label = labels_batch[0].numpy()\n",
        "\n",
        "  print(\"First image to predict\")\n",
        "  plt.imshow(first_image)\n",
        "  print(\"Actual label:\", class_names[first_label])\n",
        "\n",
        "  # Create a predicted image\n",
        "  batch_prediction = model.predict(image_batch) # Prediction for 32 labels\n",
        "  print(f\"Predicted label: {class_names[np.argmax(batch_prediction[0])]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADPxPg5wEt89"
      },
      "source": [
        "# build a fuction that takes model and image as input \n",
        "# and returns predicted class and confidence\n",
        "def predict(model, img):\n",
        "  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy()) # convert an image into image array\n",
        "  img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "  predictions = model.predict(img_array) # call predict function to predict the image array\n",
        "\n",
        "  predicted_class = class_names[np.argmax(predictions[0])] # get the best predicted class\n",
        "  confidence = round(100 * (np.max(predictions[0])), 2) # create a confidence for the predictions\n",
        "  return predicted_class, confidence # Confidence means accuracy of the prediction is 100%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09hDUPcLFdQR"
      },
      "source": [
        "# predict 9 images using the function above\n",
        "plt.figure(figsize=(15,17)) # Increase the figure size\n",
        "for images, labels in test_path.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3,3, i + 1) # Use a subplot function to display all the nine predicted images\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    \n",
        "    predicted_class, confidence = predict(model, images[i].numpy())\n",
        "    actual_class = class_names[labels[i]]\n",
        "\n",
        "    plt.title(f\"Actual: {actual_class}, \\n Predicted: {predicted_class}. \\n Confidence: {confidence}%\")\n",
        "\n",
        "    plt.axis = \"off\" # Remove the axis values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FISjkNZEF4s4"
      },
      "source": [
        "# Create a folder in your local machine and save your model using the code below\n",
        "model_version=1\n",
        "model.save(f\"../models{model_version}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWVyiEKEGNkG"
      },
      "source": [
        "# Saving the model\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "model.save('MobileNet_Epoch_20')\n",
        "model.save('MobileNet_Epoch_20.h5')\n",
        "model.save('MobileNet_Epoch_20.js')\n",
        "model.save('MobileNet_Epoch_20.pd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnSPZ4TELA2W"
      },
      "source": [
        "* We'll be using this to build a new model. This new model will consist of the original MobileNet up to the sixth to last layer. We're not including the last five layers of the original MobileNet.\n",
        "\n",
        "* By looking at the summary of the original model, we can see that by not including the last five layers, we'll be including everything up to and including the last global_average_pooling layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0LNfyrLA2X"
      },
      "source": [
        "#Next, we're going to grab the output from the sixth to last layer of the model and store it in this variable x.\n",
        "#his setup, we'll be keeping the vast majority of the original MobileNet architecutre, which has 88 layers total.\n",
        "\n",
        "x = mobile.layers[-6].output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrwkA4tTLA2X"
      },
      "source": [
        "#### **create an output layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFy6DC9YLA2Y"
      },
      "source": [
        "#create an output layer that we're calling output, \n",
        "#which will just be a Dense layer with 6 output nodes for the ten corresponding classes,\n",
        "# we'll use the softmax activation function.\n",
        "\n",
        "output = Dense(units=6, activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eucNnwCTLA2Y"
      },
      "source": [
        "#construct the new fine-tuned model\n",
        "model = keras.Model(inputs=mobile.input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TJTVDUWLA2Y"
      },
      "source": [
        "* To build the new model, we create an instance of the Model class and specify the inputs to the model to be equal to the input of the original MobileNet\n",
        "* Then we define the outputs of the model to be equal to the output variable we created directly above.\n",
        "* This creates a new model, which is identical to the original MobileNet up to the original model's sixth to last layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9F4AV_6LA2Z"
      },
      "source": [
        "#choose how many layers we actually want to be trained when we train on our new data set.\n",
        "\n",
        "#we train the last 10 layers\n",
        "for layer in model.layers[:-10]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWNXlZRILA2Z"
      },
      "source": [
        "* So the twenty-third-to-last layer and all layers after it will be trained when we fit the model on the new data set. All layers above will not be trained, so their original ImageNet weights will stay in place.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLNNXlATLA2a",
        "outputId": "79eb7d3f-d3d0-4ede-9780-857df9ecd7d8"
      },
      "source": [
        "#Looking at the model summary to see the new model architecture, along with how the number of trainable parameters.\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 3,235,014\n",
            "Trainable params: 1,070,086\n",
            "Non-trainable params: 2,164,928\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL7TFZmqLA2a",
        "outputId": "a89bd60b-e4f0-45c8-adbf-d1ef3764d071"
      },
      "source": [
        "#compile the model\n",
        "#wandb.init(project='CNN Classification Model',save_code= True, name='MobileNet_basemodel') \n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer, loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oMjIDP0LA2b"
      },
      "source": [
        "# fit() to train the model\n",
        "run = wandb.init(project='CNN Classification Model',save_code= True, name='MobileNet_Bal_model',\n",
        "            config={  # and include hyperparameters and metadata\n",
        "                     \"learning_rate\": 0.0001,\n",
        "                     \"epochs\": 10,\n",
        "                    \"optimizer\":\"Adam\",\n",
        "                    \"metric\": 'Accuracy',\n",
        "                     \"loss_function\": \"categorical_crossentropy\", #CategoricalCrossentropy\n",
        "                     \"architecture\": \"ModelNet Model\",\n",
        "                     \"dataset\": \"Maize Crop images\"\n",
        "                 })\n",
        "config = wandb.config \n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=config.learning_rate)\n",
        "model.compile(optimizer, loss=config.loss_function, metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(x=train_batches,\n",
        "            steps_per_epoch=len(train_batches),\n",
        "            validation_data=valid_batches,\n",
        "            validation_steps=len(valid_batches),\n",
        "            epochs=config.epochs, callbacks=[WandbCallback()],\n",
        "            verbose=2\n",
        ")\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol5UIKKjW6no"
      },
      "source": [
        "test_labels_2 = test_path.classes\n",
        "predictions_2 = model.predict(x=test_batches, steps=len(test_path), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx1x_Fg_JBOP"
      },
      "source": [
        "tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=None, alpha=1.0, minimalistic=False, include_top=True,\n",
        "    weights='imagenet', input_tensor=None, classes=1000, pooling=None,\n",
        "    dropout_rate=0.2, classifier_activation='softmax',\n",
        "    include_preprocessing=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_6bNgQCsirK"
      },
      "source": [
        "### Increase Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "146f95f1af3a4eea8e5a0d0a2af40d0a"
          ]
        },
        "id": "slLasQ0ZViFc",
        "outputId": "c393d774-e74a-4371-b443-58a9d061de75"
      },
      "source": [
        "# fit() to train the model\n",
        "run = wandb.init(project='CNN Classification Model',save_code= True, name='MobileNet_Bal_model_epoch50',\n",
        "            config={  # and include hyperparameters and metadata\n",
        "                     \"learning_rate\": 0.0001,\n",
        "                     \"epochs\": 50,\n",
        "                    \"optimizer\":\"Adam\",\n",
        "                    \"metric\": 'Accuracy',\n",
        "                     \"loss_function\": \"categorical_crossentropy\", #CategoricalCrossentropy\n",
        "                     \"architecture\": \"ModelNet Model\",\n",
        "                     \"dataset\": \"Maize Crop images\"\n",
        "                 })\n",
        "config = wandb.config \n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=config.learning_rate)\n",
        "model.compile(optimizer, loss=config.loss_function, metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(x=train_batches,\n",
        "            steps_per_epoch=len(train_batches),\n",
        "            validation_data=valid_batches,\n",
        "            validation_steps=len(valid_batches),\n",
        "            epochs=config.epochs, callbacks=[WandbCallback()],\n",
        "            verbose=2\n",
        ")\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">MobileNet_Bal_model_epoch50</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2na7sxdk\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2na7sxdk</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210926_190130-2na7sxdk</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "300/300 - 27s - loss: 1.6690 - acc: 0.6950 - val_loss: 1.3431 - val_acc: 0.7530\n",
            "Epoch 2/50\n",
            "300/300 - 24s - loss: 1.0148 - acc: 0.7500 - val_loss: 1.0265 - val_acc: 0.7879\n",
            "Epoch 3/50\n",
            "300/300 - 24s - loss: 0.8067 - acc: 0.7807 - val_loss: 0.8043 - val_acc: 0.8061\n",
            "Epoch 4/50\n",
            "300/300 - 23s - loss: 0.6029 - acc: 0.8083 - val_loss: 0.6877 - val_acc: 0.8152\n",
            "Epoch 5/50\n",
            "300/300 - 23s - loss: 0.5315 - acc: 0.8300 - val_loss: 0.5913 - val_acc: 0.8258\n",
            "Epoch 6/50\n",
            "300/300 - 23s - loss: 0.4649 - acc: 0.8437 - val_loss: 0.5325 - val_acc: 0.8424\n",
            "Epoch 7/50\n",
            "300/300 - 23s - loss: 0.3935 - acc: 0.8567 - val_loss: 0.4851 - val_acc: 0.8485\n",
            "Epoch 8/50\n",
            "300/300 - 23s - loss: 0.3672 - acc: 0.8600 - val_loss: 0.4446 - val_acc: 0.8530\n",
            "Epoch 9/50\n",
            "300/300 - 23s - loss: 0.3212 - acc: 0.8767 - val_loss: 0.4272 - val_acc: 0.8515\n",
            "Epoch 10/50\n",
            "300/300 - 23s - loss: 0.2943 - acc: 0.8867 - val_loss: 0.3976 - val_acc: 0.8606\n",
            "Epoch 11/50\n",
            "300/300 - 23s - loss: 0.2568 - acc: 0.9087 - val_loss: 0.3821 - val_acc: 0.8636\n",
            "Epoch 12/50\n",
            "300/300 - 23s - loss: 0.2416 - acc: 0.9083 - val_loss: 0.3700 - val_acc: 0.8712\n",
            "Epoch 13/50\n",
            "300/300 - 23s - loss: 0.2235 - acc: 0.9177 - val_loss: 0.3551 - val_acc: 0.8667\n",
            "Epoch 14/50\n",
            "300/300 - 23s - loss: 0.2025 - acc: 0.9273 - val_loss: 0.3544 - val_acc: 0.8682\n",
            "Epoch 15/50\n",
            "300/300 - 23s - loss: 0.1905 - acc: 0.9263 - val_loss: 0.3494 - val_acc: 0.8712\n",
            "Epoch 16/50\n",
            "300/300 - 23s - loss: 0.1747 - acc: 0.9357 - val_loss: 0.3442 - val_acc: 0.8758\n",
            "Epoch 17/50\n",
            "300/300 - 23s - loss: 0.1607 - acc: 0.9423 - val_loss: 0.3355 - val_acc: 0.8773\n",
            "Epoch 18/50\n",
            "300/300 - 23s - loss: 0.1443 - acc: 0.9473 - val_loss: 0.3238 - val_acc: 0.8788\n",
            "Epoch 19/50\n",
            "300/300 - 23s - loss: 0.1330 - acc: 0.9583 - val_loss: 0.3252 - val_acc: 0.8788\n",
            "Epoch 20/50\n",
            "300/300 - 24s - loss: 0.1250 - acc: 0.9600 - val_loss: 0.3340 - val_acc: 0.8818\n",
            "Epoch 21/50\n",
            "300/300 - 24s - loss: 0.1099 - acc: 0.9653 - val_loss: 0.3291 - val_acc: 0.8848\n",
            "Epoch 22/50\n",
            "300/300 - 24s - loss: 0.0930 - acc: 0.9700 - val_loss: 0.3213 - val_acc: 0.8848\n",
            "Epoch 23/50\n",
            "300/300 - 24s - loss: 0.0910 - acc: 0.9693 - val_loss: 0.3167 - val_acc: 0.8788\n",
            "Epoch 24/50\n",
            "300/300 - 24s - loss: 0.0866 - acc: 0.9707 - val_loss: 0.3239 - val_acc: 0.8864\n",
            "Epoch 25/50\n",
            "300/300 - 24s - loss: 0.0752 - acc: 0.9747 - val_loss: 0.3279 - val_acc: 0.8879\n",
            "Epoch 26/50\n",
            "300/300 - 24s - loss: 0.0648 - acc: 0.9817 - val_loss: 0.3448 - val_acc: 0.8818\n",
            "Epoch 27/50\n",
            "300/300 - 24s - loss: 0.0556 - acc: 0.9847 - val_loss: 0.3411 - val_acc: 0.8864\n",
            "Epoch 28/50\n",
            "300/300 - 24s - loss: 0.0550 - acc: 0.9860 - val_loss: 0.3430 - val_acc: 0.8864\n",
            "Epoch 29/50\n",
            "300/300 - 24s - loss: 0.0562 - acc: 0.9837 - val_loss: 0.3415 - val_acc: 0.8924\n",
            "Epoch 30/50\n",
            "300/300 - 24s - loss: 0.0442 - acc: 0.9873 - val_loss: 0.3450 - val_acc: 0.8955\n",
            "Epoch 31/50\n",
            "300/300 - 24s - loss: 0.0493 - acc: 0.9853 - val_loss: 0.3573 - val_acc: 0.8955\n",
            "Epoch 32/50\n",
            "300/300 - 24s - loss: 0.0343 - acc: 0.9910 - val_loss: 0.3516 - val_acc: 0.8939\n",
            "Epoch 33/50\n",
            "300/300 - 24s - loss: 0.0354 - acc: 0.9897 - val_loss: 0.3425 - val_acc: 0.8939\n",
            "Epoch 34/50\n",
            "300/300 - 24s - loss: 0.0353 - acc: 0.9873 - val_loss: 0.3526 - val_acc: 0.9030\n",
            "Epoch 35/50\n",
            "300/300 - 24s - loss: 0.0299 - acc: 0.9910 - val_loss: 0.3557 - val_acc: 0.8970\n",
            "Epoch 36/50\n",
            "300/300 - 24s - loss: 0.0314 - acc: 0.9920 - val_loss: 0.3551 - val_acc: 0.9000\n",
            "Epoch 37/50\n",
            "300/300 - 24s - loss: 0.0236 - acc: 0.9937 - val_loss: 0.3595 - val_acc: 0.8985\n",
            "Epoch 38/50\n",
            "300/300 - 24s - loss: 0.0242 - acc: 0.9933 - val_loss: 0.3605 - val_acc: 0.9015\n",
            "Epoch 39/50\n",
            "300/300 - 24s - loss: 0.0261 - acc: 0.9933 - val_loss: 0.3587 - val_acc: 0.8985\n",
            "Epoch 40/50\n",
            "300/300 - 24s - loss: 0.0350 - acc: 0.9923 - val_loss: 0.3604 - val_acc: 0.9030\n",
            "Epoch 41/50\n",
            "300/300 - 24s - loss: 0.0333 - acc: 0.9877 - val_loss: 0.3725 - val_acc: 0.8955\n",
            "Epoch 42/50\n",
            "300/300 - 23s - loss: 0.0260 - acc: 0.9940 - val_loss: 0.3718 - val_acc: 0.9015\n",
            "Epoch 43/50\n",
            "300/300 - 23s - loss: 0.0242 - acc: 0.9913 - val_loss: 0.3685 - val_acc: 0.8955\n",
            "Epoch 44/50\n",
            "300/300 - 23s - loss: 0.0194 - acc: 0.9937 - val_loss: 0.3816 - val_acc: 0.8985\n",
            "Epoch 45/50\n",
            "300/300 - 23s - loss: 0.0229 - acc: 0.9920 - val_loss: 0.3783 - val_acc: 0.9030\n",
            "Epoch 46/50\n",
            "300/300 - 23s - loss: 0.0206 - acc: 0.9937 - val_loss: 0.3859 - val_acc: 0.8970\n",
            "Epoch 47/50\n",
            "300/300 - 23s - loss: 0.0199 - acc: 0.9943 - val_loss: 0.3863 - val_acc: 0.9045\n",
            "Epoch 48/50\n",
            "300/300 - 23s - loss: 0.0252 - acc: 0.9930 - val_loss: 0.3680 - val_acc: 0.9091\n",
            "Epoch 49/50\n",
            "300/300 - 23s - loss: 0.0172 - acc: 0.9943 - val_loss: 0.3710 - val_acc: 0.9015\n",
            "Epoch 50/50\n",
            "300/300 - 23s - loss: 0.0211 - acc: 0.9933 - val_loss: 0.3888 - val_acc: 0.8985\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1286<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "146f95f1af3a4eea8e5a0d0a2af40d0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 27.08MB of 27.08MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210926_190130-2na7sxdk/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210926_190130-2na7sxdk/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>acc</td><td>0.99333</td></tr><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.31673</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>loss</td><td>0.02113</td></tr><tr><td>val_acc</td><td>0.89848</td></tr><tr><td>val_loss</td><td>0.38882</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>acc</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇█▇█▇███</td></tr><tr><td>val_loss</td><td>█▆▄▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">MobileNet_Bal_model_epoch50</strong>: <a href=\"https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2na7sxdk\" target=\"_blank\">https://wandb.ai/mo_g/CNN%20Classification%20Model/runs/2na7sxdk</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1zJO5Ks9uP"
      },
      "source": [
        "### Model confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKPbFrhhs9uR"
      },
      "source": [
        "test_labels = test_batches.classes\n",
        "predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2tJAnpJs9uT"
      },
      "source": [
        "cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))\n",
        "#wandb.log({'confusion_matrix':cm})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSk4C_qLs9uU",
        "outputId": "9754281b-58f9-460c-ed47-50a4b79b4304"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "test_batches.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Blight': 0,\n",
              " 'Common_rust': 1,\n",
              " 'Gray Leaf Spot': 2,\n",
              " 'SMUT500': 3,\n",
              " 'healthy': 4,\n",
              " 'maizestreak_aug': 5}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "vN49Vc_ws9uW",
        "outputId": "fbb20282-615c-4a21-dbf1-5ec9848380a0"
      },
      "source": [
        "cm_plot_labels = ['0','1','2','3','4','5']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[22  1  4  0  0  3]\n",
            " [ 1 25  4  0  0  0]\n",
            " [ 1  2 27  0  0  0]\n",
            " [ 0  0  0 29  0  1]\n",
            " [ 0  0  0  0 30  0]\n",
            " [ 0  0  0  0  0 30]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oe_xuHts9uX",
        "outputId": "b038039b-f0da-4615-b83a-dc21c6b82d58"
      },
      "source": [
        "#Classification report\n",
        "print(classification_report(y_true=test_labels, y_pred=predictions.argmax(axis=1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.73      0.81        30\n",
            "           1       0.89      0.83      0.86        30\n",
            "           2       0.77      0.90      0.83        30\n",
            "           3       1.00      0.97      0.98        30\n",
            "           4       1.00      1.00      1.00        30\n",
            "           5       0.88      1.00      0.94        30\n",
            "\n",
            "    accuracy                           0.91       180\n",
            "   macro avg       0.91      0.91      0.90       180\n",
            "weighted avg       0.91      0.91      0.90       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcQf0splhJf"
      },
      "source": [
        "we saved the best model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCpKFlbwkQwK",
        "outputId": "22b9b856-e921-4aee-95b0-79a9d76c21b0"
      },
      "source": [
        "#saving the model\n",
        "from keras.models import load_model\n",
        "\n",
        "model.save('mobile_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: mobile_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYb1-w4iP5Xs"
      },
      "source": [
        "# Vit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O_LxRkDP7Az"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QWIfGJBntH6"
      },
      "source": [
        "# Libraries & Data Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_UJaXsGoF0N"
      },
      "source": [
        "%%capture\n",
        "!pip install einops\n",
        "!pip install transformers\n",
        "!pip install vit-pytorch linformer\n",
        "\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXLXb-Si7aES"
      },
      "source": [
        "# import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "\n",
        "# wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdkfx22Us3ad"
      },
      "source": [
        "[resource](https://github.com/pytorch/vision/blob/6b071be9da221998c9ba4173fb5e4186dabdc9fe/torchvision/datasets/folder.py#L65) , [optimizers](https://pytorch.org/docs/stable/optim.html), [HuggiFace](https://huggingface.co/transformers/model_doc/vit.html), [Guide](https://colab.research.google.com/drive/1Y_RkKMr-pxGfoYbqUoYlE6HW3WHZDOFx), [Fine-tuning](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb#scrollTo=HimhtFgIxEfg)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC6A-VcmnP8L"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import einops\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# loading Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(1000)\n",
        "\n",
        "# import Linformer\n",
        "from linformer import Linformer   \n",
        "import glob   \n",
        "from PIL import Image\n",
        "from itertools import chain   \n",
        "from vit_pytorch.efficient import ViT   \n",
        "from tqdm.notebook import tqdm   \n",
        "from __future__ import print_function\n",
        "\n",
        "# import torch and related libraries\n",
        "import torch   \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms   \n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR  \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms \n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "#sklearn to split the data\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKvyWO_FvoBe"
      },
      "source": [
        "## Position Embedding\n",
        "\n",
        "* So far, the model has no idea about the original position of the patches.\n",
        "* We need to pass this spatial information. \n",
        "* This can be done in different ways, in ViT we let the model learn it. \n",
        "* The position embedding is just a tensor of shape N_PATCHES + 1 (token), EMBED_SIZE that is added to the projected patches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwqqUzSUv3tY",
        "outputId": "e7de1722-9e97-4291-a96c-28dfec050e8c"
      },
      "source": [
        "# class PatchEmbedding(nn.Module):\n",
        "#     def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
        "#         self.patch_size = patch_size\n",
        "#         super().__init__()\n",
        "#         self.projection = nn.Sequential(\n",
        "#             # using a conv layer instead of a linear one -> performance gains\n",
        "#             nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "#             Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "#         )\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "#         self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "\n",
        "        \n",
        "#     def forward(self, x: Tensor) -> Tensor:\n",
        "#         b, _, _, _ = x.shape\n",
        "#         x = self.projection(x)\n",
        "#         cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "#         # prepend the cls token to the input\n",
        "#         x = torch.cat([cls_tokens, x], dim=1)\n",
        "#         # add position embedding\n",
        "#         x += self.positions\n",
        "#         return x\n",
        "    \n",
        "# PatchEmbedding()(x).shape\n",
        "\n",
        "# #We added the position embedding in the .positions field and sum it to the patches in the .forward function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W8yYkD_wMPO"
      },
      "source": [
        "# To implement Transformer in ViT only the Encoder is used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJREN32UOp2g"
      },
      "source": [
        "# Linformer\n",
        "      * Adam Optimizer\n",
        "      * lr = 3e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T69-melLOt6E"
      },
      "source": [
        "#Install the ViT PyTorch package and Linformer\n",
        "%%capture\n",
        "!pip install vit-pytorch linformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3heGaa00O76w"
      },
      "source": [
        "# loading Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import Linformer\n",
        "from linformer import Linformer   \n",
        "import glob   \n",
        "from PIL import Image\n",
        "from itertools import chain   \n",
        "from vit_pytorch.efficient import ViT   \n",
        "from tqdm.notebook import tqdm   \n",
        "from __future__ import print_function\n",
        "\n",
        "# import torch and related libraries\n",
        "import torch   \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms   \n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR  \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "#sklearn to split the data\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp8vyAiHPdW4"
      },
      "source": [
        "#definining batch size, epocs, learning rate and gamma for training \n",
        "\n",
        "batch_size = 12     #***\n",
        "epochs = 10\n",
        "lr = 3e-5\n",
        "gamma = 0.7 #for learning rate scheduler  #***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "Q90HhWZ0PkFA",
        "outputId": "1ee7af0b-9ce6-4663-83c7-dedd48189d60"
      },
      "source": [
        "#Load data\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/train'\n",
        "test_dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/test'\n",
        "valid_dir = '/content/drive/MyDrive/Data Science/Marine Invertebrates Classification Challenge/Data/augmented_split/val'\n",
        "\n",
        "\n",
        "# Torch transforms\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "\n",
        "valid_data = datasets.ImageFolder(valid_dir, transform=val_transforms)\n",
        "\n",
        "test_data = datasets.ImageFolder(test_dir, transform=val_transforms)\n",
        "\n",
        "\n",
        "print(f\"Train Data: {len(train_data)}\")\n",
        "print(f\"Test Data: {len(valid_data)}\")\n",
        "print(f\"Test Data: {len(test_data)}\")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-731461fca906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes Actinoscyphia_plebeia, Aequorea_spp, Africolaria_rutila, Amalda_bullioides, Aphelodoris_sp_, Aphrodita_alta, Armina_sp_, Ascidiacea, Astropecten_irregularis_pontoporeus, Athleta_abyssicola, Bryozoa, Cavernularia_spp, Cephalodiscus_gilchristi, Charonia_lampas, Cheilostomatida, Cheiraster_hirsutus, Chrysaora_fulgida, Comanthus_wahlbergii, Comitas_saldanhae, Cosmasterias_felipes, Crossaster_penicillatus, Echinus_gilchristi, Eleutherobia_variable, Euspira_napus, Exodromidia_spinosa, Exodromidia_spinosissima, Flabellum_(Ulocyathus)_messum, Funchalia_woodwardi, Fusinus_hayesi, Fusivoluta_pyrrhostoma, Glyphocrangon_spp, Goneplax_clevai, Granulifusus_rubrolineatus, Gynandrocarpa_placenta, Halcurias_capensis, Haliporoides_triarthrus, Hemiocnus_insolens, Henricia_abyssalis, Hermit_crab, Holothuroidea, Inachidae, Isididae, Isopods, Kaloplocamus_ramosus, Limopsis_chuni, Lithodes_ferox, Lophaster_quadrispinus, Marginella_musica, Marthasterias_africana, Mediaster_bairdi_capensis, Merhippolyte_agulhasensis, Miersiograpsus_kingsleyi, Munida_benguela, Mursia_cristiata, Mycale_anisochela, Nassarius speciosus, Nassarius_vinctus, Neolithodes_asperrimus, Neopilumnoplax_heterochir, Nudibranchia, Ophiomyxa_vivipara_capensis, Ophiothrix_aristulata, Ophiothrix_fragilis, Ophiura_trimeni, Parapagurus_bouvieri, Pasiphaea_sp._1, Pasiphaea_sp._2, Pecten_sulcicostatus, Pelagia_noctiluca, Perissasterias_polyacantha, Philine_aperta, Philino..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7-SrWaD8qyZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgksUOUdjndg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dec33ef3-9448-4451-b6fe-36384be2d547"
      },
      "source": [
        "imagespath = '/content/drive/MyDrive/MOG/DATA/Datasets/Augment Images/train/Blight'\n",
        "img = \n",
        "for filename in sorted(os.listdir(imagespath)):\n",
        "  if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".JPEG\") or filename.endswith(\".JPG\"):\n",
        "    label = imagespath.split('/')[-2]\n",
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train'"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFePRAkX1std"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 3e-5\n",
        "gamma = 0.7 #for learning rate scheduler "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_D5Mzo1ydu"
      },
      "source": [
        "efficient_transformer = Linformer(\n",
        "    dim=128,\n",
        "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
        "    depth=12, #***\n",
        "    heads=8,#***\n",
        "    k=64  #2nd dimention of the p_bar matrix\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjHga0Pl12wO"
      },
      "source": [
        "#Visual transformer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViT(\n",
        "    dim=128,\n",
        "    image_size=224,\n",
        "    patch_size=32,  #***\n",
        "    num_classes=6,\n",
        "    transformer=efficient_transformer,\n",
        "    channels=3,\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWiUsWYI18LP"
      },
      "source": [
        "# loss function\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# scheduler\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APTHvhbp2AZF"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    for data, label in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in valid_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(valid_loader)\n",
        "            epoch_val_loss += val_loss / len(valid_loader)\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfAtBNAi2AUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}